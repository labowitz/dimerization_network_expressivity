{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49aea335-a6e4-4b8f-8a64-831f185efce0",
   "metadata": {},
   "source": [
    "# Generate Parameter Screen\n",
    "\n",
    "Author: Jacob Parres-Gold (jacobparresgold@gmail.com)\n",
    "\n",
    "Last Revised: 20231019\n",
    "\n",
    "This notebook performs a parameter screen of networks, considering 1 or 2 monomers as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e3348a-6b64-48d9-85da-9f76e73cdb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import scipy.stats\n",
    "import sys\n",
    "\n",
    "# EQTK to calculate equilibrium concentrations - install instructions here https://eqtk.github.io/getting_started/eqtk_installation.html\n",
    "import eqtk\n",
    "\n",
    "# Ray for parallelization - install instructions here https://docs.ray.io/en/latest/ray-overview/installation.html\n",
    "import ray\n",
    "\n",
    "# Networkx for graphs - install instructions here https://networkx.org/documentation/stable/install.html\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ac06aa-0570-4d78-9b42-f2a218c64a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load utility functions from dimer_network_utilities.py\n",
    "utilities_path = str(pathlib.Path('C:\\\\Users','jacob','Local_Coding',\\\n",
    "                                  'ElowitzLab','Promiscuous_Networks','Notebooks','Paper_Notebooks')) # Directory of your utilities.py file\n",
    "sys.path.append(utilities_path)\n",
    "from dimer_network_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1493d22-5ef2-4cfc-a562-75fabbe87506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ray\n",
    "_ = ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ec75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To shut down ray:\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75920364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package versions:\n",
      "python=3.8.13\n",
      "eqtk=0.1.2\n",
      "matplotlib=3.5.1\n",
      "networkx=2.7.1\n",
      "numpy=1.21.5\n",
      "pandas=1.4.1\n",
      "scipy=1.10.1\n"
     ]
    }
   ],
   "source": [
    "# Print package versions\n",
    "import pkg_resources\n",
    "imports = list(set(get_imports()))\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "print(\"Package versions:\")\n",
    "print(\"python={}\".format(sys.version.split('|')[0].replace(' ','')))\n",
    "for r in requirements:\n",
    "    print(\"{}={}\".format(*r))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8aa6c11d",
   "metadata": {},
   "source": [
    "## Functions for Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8ad99a-3bf7-49a4-b71a-48b26433ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eqtk(N, C0, params, acc_monomer_ind):\n",
    "    \"\"\"\n",
    "    Run eqtk.solve given the input stoichiometry matrix, initial concentrations, and parameters.\n",
    "\n",
    "    See eqtk.solve documentation for more details on the syntax.\n",
    "    (https://eqtk.github.io/user_guide/generated/highlevel/eqtk.solve.html)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : Array-like, shape (num_rxns, num_species)\n",
    "        Input stoichiometry matrix\n",
    "    C0 : Array-like, shape (num_simulation_points, num_species)\n",
    "        Initial concentrations of all species for each simulation point.\n",
    "        Accessory monomer levels will be set based on params.\n",
    "    params : List-like, len num_combos_with_replacement(m,2) + (m-1)\n",
    "        Parameters including Kij values and accessory monomer levels\n",
    "    acc_monomer_ind : int  \n",
    "        Index of accessory monomers in species list\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    C: Array-like\n",
    "        Equilibrium concentrations of all species\n",
    "    \"\"\"\n",
    "    num_rxns = N.shape[0]\n",
    "    # Extract Kij values\n",
    "    K = params[:num_rxns]\n",
    "    # Set accessory monomer levels\n",
    "    C0[:,acc_monomer_ind] = params[num_rxns:]\n",
    "\n",
    "    return eqtk.solve(c0=C0, N=N, K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6dfb41-4792-43e3-8781-622b91c813bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def simulate_networks(m, num_inputs, param_sets, t = 12, input_lb = -3, input_ub = 3):\n",
    "    \"\"\"\n",
    "    Run simulations for dimer networks, over a titration of concentrations for each input monomer. \n",
    "    When >1 input monomers are varied (e.g., a 2D matrix), the first monomer is sampled outermost, \n",
    "    while the second is sampled inner to that.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int. \n",
    "        Number of monomer species in the network.\n",
    "    num_inputs: int\n",
    "        Number of monomers to count as inputs (those titrated). \n",
    "        E.g., if num_inputs=2, the first two monomers will be treated as inputs.\n",
    "    t : int. Default 12. \n",
    "        Number of values to titrate each input monomer species. \n",
    "        Values spaced evenly on a log10 scale\n",
    "    input_lb : int. Default -3\n",
    "        lower bound for titrating the input monomer species. Log10 scale\n",
    "    input_ub : int. Default 3\n",
    "        upper bound for titrating the input monomer species.Llog10 scale\n",
    "    param_sets : Array-like, shape (num_sets, num_parameters)\n",
    "        Parameter sets for simulating multiple dimerization networks.  \n",
    "    Returns\n",
    "    -------\n",
    "    C0 : Array-like, shape (t, number of species)\n",
    "        Initial concentration array used for eqtk.solve\n",
    "    S_all : Array-like, shape (t, number of species, num_sets) \n",
    "        Equlibrium concentration of all species\n",
    "        for all parameter sets and each input titration point.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Create stoichiometry matrix \n",
    "    num_sets = param_sets.shape[0]\n",
    "    N = make_nXn_stoich_matrix(m)\n",
    "    num_rxns = N.shape[0]\n",
    "\n",
    "    # Create initial concentration array\n",
    "    M0_min = [input_lb]*num_inputs + [0] * (m-num_inputs) # Species concentrations at min of inputs\n",
    "    M0_max = [input_ub]*num_inputs + [0] * (m-num_inputs) # Species concentrations at max of inputs\n",
    "    num_conc = [t]*num_inputs + [1] * (m-num_inputs) # Number of concentrations to titrate for each species\n",
    "    C0 = make_C0_grid(m, M0_min=M0_min, M0_max=M0_max, num_conc=num_conc) \n",
    "        \n",
    "    acc_monomer_ind = np.arange(num_inputs,m) # Indices of accessory monomers\n",
    "    S_all = np.zeros((C0.shape[0], C0.shape[1], num_sets))\n",
    "    # For each parameter set, run eqtk.solve\n",
    "    for pset_index, pset in enumerate(param_sets):\n",
    "        S_all[:,:,pset_index] = run_eqtk(N, C0.copy(), pset, acc_monomer_ind)\n",
    "    return C0,S_all  \n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94849f32-d892-489b-8e31-744a0f640b75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions for 1D Screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f3fd1a-601a-4ccd-86a0-4ffea67dcef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_param_sets(m,num_sets,K_log_range,A_log_range,null_K=1e-10,null_K_homo_fraction=0.25,seed=42):\n",
    "    '''\n",
    "    Sample parameter sets on a continuous log-uniform range (with Latin Hypercube), making sure to sample evenly across networks of different connectivities\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m : int\n",
    "        Number of monomers in the network\n",
    "    num_sets : int\n",
    "        Number of parameter sets to sample\n",
    "    K_log_range : list-like, len 2\n",
    "        Lower and upper bounds for log10 of Kij values\n",
    "    A_log_range : list-like, len 2\n",
    "        Lower and upper bounds for log10 of accessory monomer levels\n",
    "    null_K : float, default 1e-10\n",
    "        Value to use for Kij values when no dimerization occurs\n",
    "        (this is done for convenience; it is easier to set K to some small value\n",
    "        than to erase the reaction when running EQTK)\n",
    "    null_K_homo_fraction : float, default 0.25\n",
    "        The fraction of homodimerization reactions that will be set to null_K (i.e., not occur)\n",
    "    seed : int, default 42\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    param_sets : Array-like, shape (num_sets, num_combos_with_replacement(m,2) + (m-1))\n",
    "    '''\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # Calculate number of parameter sets to create, such\n",
    "    # that different connectivities are sampled evenly\n",
    "    num_possible_num_edges = num_combos(m,r=2) - (m-1) + 1\n",
    "    num_sets_per_num_edges = math.ceil(num_sets/num_possible_num_edges)\n",
    "    num_sets = int(num_sets_per_num_edges*num_possible_num_edges)\n",
    "\n",
    "    # Create keys for which dimers are homodimers vs. heterodimers\n",
    "    edge_key = {}\n",
    "    homo_edges = []\n",
    "    hetero_edges = []\n",
    "\n",
    "    for i,combo in enumerate(itertools.combinations_with_replacement(range(m),r=2)):\n",
    "        edge_key[(combo[0],combo[1])] = i\n",
    "        if combo[0]==combo[1]:\n",
    "            homo_edges.append(i)\n",
    "        else:\n",
    "            hetero_edges.append(i)\n",
    "\n",
    "    param_sets = np.concatenate((np.full((num_sets,num_combos_with_replacement(m,r=2)),null_K),np.zeros((num_sets,m-1))),axis=1)\n",
    "    max_num_edges_for_disconnected = math.comb(m-1,2)\n",
    "\n",
    "    for num_edges_i,num_edges in enumerate(range(m-1,num_combos(m,r=2)+1)):\n",
    "        # For different connectivities (number of edges), do the following:\n",
    "        for param_set in range(num_sets_per_num_edges):\n",
    "            # For each parameter set, do the following:\n",
    "            graph = nx.gnm_random_graph(n=m,m=num_edges) # Generate a Networkx graph\n",
    "            if num_edges<=max_num_edges_for_disconnected:\n",
    "                # If false, graph is always connected\n",
    "                while not nx.is_connected(graph):\n",
    "                    # Regenerate graph until connected\n",
    "                    graph = nx.gnm_random_graph(n=m,m=num_edges)\n",
    "            edge_indices = [edge_key[x] for x in graph.edges]\n",
    "            param_sets[(num_edges_i*num_sets_per_num_edges)+param_set,edge_indices] = 1\n",
    "        # Fill all heterodimer K's with hypercube sampling\n",
    "        lhs_sampler =  scipy.stats.qmc.LatinHypercube(d=num_edges, centered=False, seed=seed)\n",
    "        hetero_Ks = lhs_sampler.random(n=num_sets_per_num_edges)\n",
    "        hetero_Ks = 10**scipy.stats.qmc.scale(hetero_Ks, K_log_range[0], K_log_range[1])\n",
    "        where_1 = np.where(param_sets[(num_edges_i*num_sets_per_num_edges):((num_edges_i+1)*num_sets_per_num_edges),:]==1)\n",
    "        param_sets[(where_1[0]+(num_edges_i*num_sets_per_num_edges),where_1[1])] = hetero_Ks.flatten()\n",
    "\n",
    "    # Generate homodimer K's with hypercube sampling\n",
    "    lhs_sampler =  scipy.stats.qmc.LatinHypercube(d=len(homo_edges), centered=False, seed=seed)\n",
    "    homo_Ks = lhs_sampler.random(n=num_sets)\n",
    "    param_sets[:,homo_edges] = 10**scipy.stats.qmc.scale(homo_Ks, K_log_range[0], K_log_range[1])\n",
    "    \n",
    "    # Fill some homodimer K's with null K\n",
    "    null_K_homo_indices = np.array(list(itertools.product(range(num_sets),homo_edges)))[rng.choice(num_sets*len(homo_edges),size=(math.ceil(null_K_homo_fraction*num_sets*len(homo_edges)),),replace=False)]\n",
    "    null_K_homo_indices = (null_K_homo_indices[:,0],null_K_homo_indices[:,1])\n",
    "    param_sets[null_K_homo_indices] = null_K\n",
    "    \n",
    "    # Generate A's with hypercube sampling\n",
    "    lhs_sampler =  scipy.stats.qmc.LatinHypercube(d=m-1, centered=False, seed=seed)\n",
    "    As = lhs_sampler.random(n=num_sets)\n",
    "    param_sets[:,-(m-1):] = 10**scipy.stats.qmc.scale(As, A_log_range[0], A_log_range[1])\n",
    "    \n",
    "    return param_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8363e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimers_from_param_set(param_set,edges_to_count,min_affinity=1e-5):\n",
    "    # Return a boolean matrix identifying whether each dimer was \"possible\" (edge present) in each network\n",
    "    return np.array([param_set[edge]>=min_affinity for edge in edges_to_count])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b49a1c4f-8ab5-4ba2-841e-fb966aa7ebdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perform 1D Screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory to save to\n",
    "out_dir = str(pathlib.Path('C:\\\\Users','jacob','Downloads'))\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 1\n",
    "K_log_range = [-5,7] # Lower and upper bounds for log10 of Kij values\n",
    "A_log_range = [-3,3] # Lower and upper bounds for log10 of accessory monomer levels\n",
    "null_K=1e-10 # Value to use for Kij values when no dimerization occurs\n",
    "                # (this is done for convenience; it is easier to set K to some small value\n",
    "                # than to erase the reaction when running EQTK)\n",
    "null_K_homo_fraction=0.25 # The fraction of homodimerization reactions that will be set to null_K (i.e., not occur)\n",
    "num_sets=int(1e6) # Approximate number of parameter sets to simulate \n",
    "                    # (more may be simulated to ensure equal numbers of parameter sets for all network connectivities)\n",
    "input_lb = -3 # Lower bound for log10 of input concentrations\n",
    "input_ub = 3 # Upper bound for log10 of input concentrations\n",
    "min_dynamic_range = 10 # Minimum dynamic range of the output responses (all other responses will be filtered out)\n",
    "round_low_concs_to = 10**-3 # Concentrations below this value will be rounded to this value\n",
    "t = 30 # Number of input concentrations to simulate\n",
    "max_curves_per_chunk_to_simulate=5000 # Maximum number of curves to simulate in a single \"chunk\"\n",
    "max_networks_per_S_all=200000 # Maximum number of networks to save in a single S_all file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3106a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_list = list(range(2,13)) # Network sizes to simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b218d521-be64-4ee9-bfb1-e7b95180e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_all_filenames_by_m = []\n",
    "all_curves_per_Sall_chunk_by_m = []\n",
    "unfiltered_curves_per_Sall_chunk_by_m = []\n",
    "nonzero_curves_per_Sall_chunk_by_m = []\n",
    "filtered_curves_per_Sall_chunk_by_m = []\n",
    "\n",
    "# Iterate over network sizes\n",
    "for m in m_list:\n",
    "    specific_out_dir = str(pathlib.Path(out_dir,f'{m}M'))\n",
    "    if not os.path.isdir(specific_out_dir):\n",
    "        os.mkdir(specific_out_dir)\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    # Sample Param Sets\n",
    "    print(f\"Sampling parameter sets for m={m}\")\n",
    "    param_sets = sample_param_sets(m=m,\\\n",
    "                               num_sets=num_sets,\\\n",
    "                               K_log_range=K_log_range,\\\n",
    "                               A_log_range=A_log_range,\\\n",
    "                               null_K=null_K,\\\n",
    "                               null_K_homo_fraction=null_K_homo_fraction,\\\n",
    "                               seed=42)\n",
    "    dimers_by_param_set = np.apply_along_axis(dimers_from_param_set,\\\n",
    "                                                  axis=1,\\\n",
    "                                                  arr=param_sets,\\\n",
    "                                                  edges_to_count=list(range(num_combos_with_replacement(m,2))))\n",
    "    num_sets = param_sets.shape[0]\n",
    "    np.save(str(pathlib.Path(specific_out_dir,f'K_log_range.npy')),np.array(K_log_range))\n",
    "    np.save(str(pathlib.Path(specific_out_dir,f'A_log_range.npy')),np.array(A_log_range))\n",
    "    np.save(str(pathlib.Path(specific_out_dir,f'K_A_param_sets.npy')), param_sets)\n",
    "    np.save(str(pathlib.Path(specific_out_dir,f'dimers_by_param_set.npy')),dimers_by_param_set)\n",
    "    # Chunk & simulate in parallel\n",
    "    # There are two levels of chunking here:\n",
    "    # 1. Each S_all matrix will only consider ~200,000 networks, for memory limitations\n",
    "    # 2. We will further chunk that up for simulation to allow parallelization\n",
    "    num_S_all_chunks = math.ceil(param_sets.shape[0]/max_networks_per_S_all)\n",
    "    S_all_processed_filtered_files = []\n",
    "    all_curves_per_Sall_chunk = []\n",
    "    unfiltered_curves_per_Sall_chunk = []\n",
    "    nonzero_curves_per_Sall_chunk = []\n",
    "    filtered_curves_per_Sall_chunk = []\n",
    "    for S_all_chunk in range(num_S_all_chunks):\n",
    "        if S_all_chunk==num_S_all_chunks:\n",
    "            param_sets_S_all_chunk = param_sets[(S_all_chunk*max_networks_per_S_all):]\n",
    "        else:\n",
    "            param_sets_S_all_chunk = param_sets[(S_all_chunk*max_networks_per_S_all):((S_all_chunk+1)*max_networks_per_S_all)]\n",
    "        print(f\"Simulating M={m}, S_all_{S_all_chunk}\")\n",
    "        num_sim_chunks = math.ceil(param_sets_S_all_chunk.shape[0]/max_curves_per_chunk_to_simulate)\n",
    "        S_all = np.zeros((t,m+num_combos_with_replacement(m,r=2),param_sets_S_all_chunk.shape[0]))\n",
    "        sim_futures = []\n",
    "        for chunk in range(num_sim_chunks):\n",
    "            if chunk==num_sim_chunks:\n",
    "                param_sets_temp = param_sets_S_all_chunk[(chunk*max_curves_per_chunk_to_simulate):]\n",
    "            else:\n",
    "                param_sets_temp = param_sets_S_all_chunk[(chunk*max_curves_per_chunk_to_simulate):((chunk+1)*max_curves_per_chunk_to_simulate)]\n",
    "            sim_futures.append(simulate_networks.remote(m = m, num_inputs = num_inputs, t = t, param_sets=param_sets_temp, input_lb = input_lb, \\\n",
    "                                input_ub = input_ub))\n",
    "        for chunk,future in zip(range(num_sim_chunks),sim_futures):\n",
    "            _, S_all_temp = ray.get(future)\n",
    "            if chunk==num_sim_chunks:\n",
    "                S_all[:,:,(chunk*max_curves_per_chunk_to_simulate):] = S_all_temp\n",
    "            else:\n",
    "                S_all[:,:,(chunk*max_curves_per_chunk_to_simulate):((chunk+1)*max_curves_per_chunk_to_simulate)] = S_all_temp\n",
    "    \n",
    "        del sim_futures\n",
    "        np.save(str(pathlib.Path(specific_out_dir,f'S_all_{S_all_chunk}.npy')), S_all)\n",
    "    \n",
    "        ###### Process data\n",
    "        S_all_processed = S_all.copy()\n",
    "        # Reorder\n",
    "        S_all_processed = S_all_processed[:,m:,:].reshape((t,-1),order='F').T # Exclude all monomers\n",
    "        # Round low concentrations\n",
    "        S_all_processed[np.where(S_all_processed<10**input_lb)] = round_low_concs_to\n",
    "        # Calculate dynamic ranges, throw out curves with low dynamic ranges\n",
    "        dynamic_ranges = np.apply_along_axis(get_dynamic_range, axis=1, arr=S_all_processed)\n",
    "        S_all_processed_filtered = S_all_processed[np.where(dynamic_ranges>min_dynamic_range)[0],:]\n",
    "        np.save(str(pathlib.Path(specific_out_dir,f'S_all_processed_filtered_{S_all_chunk}.npy')), S_all_processed_filtered)\n",
    "        S_all_processed_filtered_files.append(str(pathlib.Path(specific_out_dir,f'S_all_processed_filtered_{S_all_chunk}.npy')))\n",
    "        all_curves_per_Sall_chunk.append(S_all_processed.shape[0])\n",
    "        if S_all_chunk==num_S_all_chunks:\n",
    "            unfiltered_curves_per_Sall_chunk.append(np.sum(dimers_by_param_set[(S_all_chunk*max_networks_per_S_all):,:]))\n",
    "        else:\n",
    "            unfiltered_curves_per_Sall_chunk.append(np.sum(dimers_by_param_set[(S_all_chunk*max_networks_per_S_all):((S_all_chunk+1)*max_networks_per_S_all),:]))\n",
    "        \n",
    "        nonzero_curves_per_Sall_chunk.append(np.where(np.all(S_all_processed==round_low_concs_to,axis=1))[0].shape[0])\n",
    "        filtered_curves_per_Sall_chunk.append(S_all_processed_filtered.shape[0])\n",
    "\n",
    "        # Calculate map between original parameter sets and those kept after filtering\n",
    "        # Allows us to trace curves in clusters back to original parameter sets\n",
    "        univ_after_filter = np.where(dynamic_ranges>=min_dynamic_range)[0]\n",
    "        num_univ_after_filter = np.where(dynamic_ranges>=min_dynamic_range)[0].shape[0]\n",
    "        univ_filter_map = np.concatenate((np.expand_dims(univ_after_filter, axis=1),np.expand_dims(np.arange(num_univ_after_filter),axis=1)),axis=1)\n",
    "        np.save(str(pathlib.Path(specific_out_dir,f'univ_filter_map_{S_all_chunk}.npy')), univ_filter_map)\n",
    "    \n",
    "    S_all_filenames_by_m.append(S_all_processed_filtered_files)\n",
    "    all_curves_per_Sall_chunk_by_m.append(all_curves_per_Sall_chunk)\n",
    "    unfiltered_curves_per_Sall_chunk_by_m.append(unfiltered_curves_per_Sall_chunk)\n",
    "    nonzero_curves_per_Sall_chunk_by_m.append(nonzero_curves_per_Sall_chunk)\n",
    "    filtered_curves_per_Sall_chunk_by_m.append(filtered_curves_per_Sall_chunk)\n",
    "\n",
    "np.save(str(pathlib.Path(out_dir,'S_all_filenames_by_m.npy')),S_all_filenames_by_m)\n",
    "np.save(str(pathlib.Path(out_dir,'all_curves_per_Sall_chunk_by_m.npy')),all_curves_per_Sall_chunk_by_m)\n",
    "np.save(str(pathlib.Path(out_dir,'unfiltered_curves_per_Sall_chunk_by_m.npy')),unfiltered_curves_per_Sall_chunk_by_m)\n",
    "np.save(str(pathlib.Path(out_dir,'nonzero_curves_per_Sall_chunk_by_m.npy')),nonzero_curves_per_Sall_chunk_by_m)\n",
    "np.save(str(pathlib.Path(out_dir,'filtered_curves_per_Sall_chunk_by_m.npy')),filtered_curves_per_Sall_chunk_by_m)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49c60bef",
   "metadata": {},
   "source": [
    "## Perform 2D Screen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7feb1823",
   "metadata": {},
   "source": [
    "For the screen of two-input functions, we will just reuse a subset of parameter sets from the 1D screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory of 1D screen to load parameter sets from\n",
    "param_set_load_dir = str(pathlib.Path('C:\\\\Users','jacob','Downloads'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df95c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory to save to\n",
    "out_dir = str(pathlib.Path('C:\\\\Users','jacob','Downloads'))\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d028b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sets_per_m_approx = 250000 # Approximate number of parameter sets to simulate for each network size\n",
    "K_log_range = [-5,7] # Lower and upper bounds for log10 of Kij values\n",
    "A_log_range = [-3,3] # Lower and upper bounds for log10 of accessory monomer concentrations\n",
    "input_lb = -3 # Lower bound for log10 of input concentrations\n",
    "input_ub = 3 # Upper bound for log10 of input concentrations\n",
    "t = 12 # Number of input concentrations to simulate\n",
    "min_dynamic_range = 10 # Minimum dynamic range of the output responses (all other responses will be filtered out)\n",
    "round_low_concs_to = 10**-3 # Concentrations below this value will be rounded to this value\n",
    "max_curves_per_chunk_to_simulate=5000 # Maximum number of curves to simulate in a single \"chunk\"\n",
    "max_networks_per_S_all=40000 # Maximum number of networks to save in a single S_all file\n",
    "\n",
    "num_inputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_list = list(range(2,13)) # Network sizes to simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ef87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_all_filenames_by_m = []\n",
    "all_curves_per_Sall_chunk_by_m = []\n",
    "unfiltered_curves_per_Sall_chunk_by_m = []\n",
    "nonzero_curves_per_Sall_chunk_by_m = []\n",
    "filtered_curves_per_Sall_chunk_by_m = []\n",
    "\n",
    "for m in m_list:\n",
    "    specific_out_dir = str(pathlib.Path(out_dir,f'{m}M'))\n",
    "    if not os.path.isdir(specific_out_dir):\n",
    "        os.mkdir(specific_out_dir)\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    # Sample Param Sets\n",
    "    print(f\"Sampling parameter sets for m={m}\")\n",
    "    all_past_param_sets = np.load(str(pathlib.Path(param_set_load_dir,f'{m}M','K_A_param_sets.npy')),allow_pickle=True)\n",
    "    num_possible_num_edges = num_combos(m,r=2) - (m-1) + 1\n",
    "    num_sets_per_num_edges_thisset = math.ceil(param_sets_per_m_approx/num_possible_num_edges)\n",
    "    num_sets_per_num_edges_fullset = math.ceil(all_past_param_sets.shape[0]/num_possible_num_edges)\n",
    "    num_sets = int(num_sets_per_num_edges_thisset*num_possible_num_edges)\n",
    "    param_sets = np.empty((num_sets,all_past_param_sets.shape[1]))\n",
    "    for num_edges_i,num_edges in enumerate(range(m-1,num_combos(m,r=2)+1)):\n",
    "        # For each connectivity level, sample a subset of parameter sets to use in the 2D screen\n",
    "        param_sets[(num_edges_i*num_sets_per_num_edges_thisset):((num_edges_i+1)*num_sets_per_num_edges_thisset),:]=\\\n",
    "                    all_past_param_sets[rng.choice(np.arange((num_edges_i*num_sets_per_num_edges_fullset),((num_edges_i+1)*num_sets_per_num_edges_fullset)),\\\n",
    "                    size=(num_sets_per_num_edges_thisset,),replace=False)]\n",
    "    # Remove M2 info\n",
    "    param_sets = np.delete(param_sets,obj=num_combos_with_replacement(m,2),axis=1)\n",
    "    dimers_by_param_set = np.apply_along_axis(dimers_from_param_set,\\\n",
    "                                                  axis=1,\\\n",
    "                                                  arr=param_sets,\\\n",
    "                                                  edges_to_count=list(range(num_combos_with_replacement(m,2))))\n",
    "    np.save(str(pathlib.Path(specific_out_dir,f'K_log_range.npy')),np.array(K_log_range))\n",
    "    np.save(str(pathlib.Path(specific_out_dir,f'A_log_range.npy')),np.array(A_log_range))\n",
    "    np.save(str(pathlib.Path(specific_out_dir,f'K_A_param_sets.npy')), param_sets)\n",
    "    np.save(str(pathlib.Path(specific_out_dir,f'dimers_by_param_set.npy')),dimers_by_param_set)\n",
    "    # Chunk & simulate in parallel\n",
    "    # There are two levels of chunking here:\n",
    "    # 1. Each S_all matrix will only consider ~40,000 networks (for memory reasons)\n",
    "    # 2. We will further chunk that up for simulation to allow parallelization\n",
    "    num_S_all_chunks = math.ceil(param_sets.shape[0]/max_networks_per_S_all)\n",
    "    S_all_processed_filtered_files = []\n",
    "    all_curves_per_Sall_chunk = []\n",
    "    unfiltered_curves_per_Sall_chunk = []\n",
    "    nonzero_curves_per_Sall_chunk = []\n",
    "    filtered_curves_per_Sall_chunk = []\n",
    "    for S_all_chunk in range(num_S_all_chunks):\n",
    "        if S_all_chunk==num_S_all_chunks:\n",
    "            param_sets_S_all_chunk = param_sets[(S_all_chunk*max_networks_per_S_all):]\n",
    "        else:\n",
    "            param_sets_S_all_chunk = param_sets[(S_all_chunk*max_networks_per_S_all):((S_all_chunk+1)*max_networks_per_S_all)]\n",
    "        print(f\"Simulating M={m}, S_all_{S_all_chunk}\")\n",
    "        num_sim_chunks = math.ceil(param_sets_S_all_chunk.shape[0]/max_curves_per_chunk_to_simulate)\n",
    "        S_all = np.zeros((t**2,m+num_combos_with_replacement(m,r=2),param_sets_S_all_chunk.shape[0]))\n",
    "        sim_futures = []\n",
    "        for chunk in range(num_sim_chunks):\n",
    "            if chunk==num_sim_chunks:\n",
    "                param_sets_temp = param_sets_S_all_chunk[(chunk*max_curves_per_chunk_to_simulate):]\n",
    "            else:\n",
    "                param_sets_temp = param_sets_S_all_chunk[(chunk*max_curves_per_chunk_to_simulate):((chunk+1)*max_curves_per_chunk_to_simulate)]\n",
    "            sim_futures.append(simulate_networks.remote(m = m, num_inputs=num_inputs,t = t, param_sets=param_sets_temp, \\\n",
    "                                                                input_lb = input_lb, input_ub = input_ub))\n",
    "        for chunk,future in zip(range(num_sim_chunks),sim_futures):\n",
    "            _, S_all_temp = ray.get(future)\n",
    "            if chunk==num_sim_chunks:\n",
    "                S_all[:,:,(chunk*max_curves_per_chunk_to_simulate):] = S_all_temp\n",
    "            else:\n",
    "                S_all[:,:,(chunk*max_curves_per_chunk_to_simulate):((chunk+1)*max_curves_per_chunk_to_simulate)] = S_all_temp\n",
    "    \n",
    "        del sim_futures\n",
    "        np.save(str(pathlib.Path(specific_out_dir,f'S_all_{S_all_chunk}.npy')), S_all)\n",
    "    \n",
    "        ###### Process data\n",
    "        S_all_processed = S_all.copy()\n",
    "        # Reorder\n",
    "        S_all_processed = S_all_processed[:,m:,:].reshape(((t**num_inputs),-1),order='F').T # Exclude all monomers\n",
    "        # Round low concentrations\n",
    "        S_all_processed[np.where(S_all_processed<10**input_lb)] = round_low_concs_to\n",
    "        # Calculate dynamic ranges, throw out curves with low dynamic ranges\n",
    "        dynamic_ranges = np.apply_along_axis(get_dynamic_range, axis=1, arr=S_all_processed)\n",
    "        S_all_processed_filtered = S_all_processed[np.where(dynamic_ranges>min_dynamic_range)[0],:]\n",
    "        np.save(str(pathlib.Path(specific_out_dir,f'S_all_processed_filtered_{S_all_chunk}.npy')), S_all_processed_filtered)\n",
    "        S_all_processed_filtered_files.append(str(pathlib.Path(specific_out_dir,f'S_all_processed_filtered_{S_all_chunk}.npy')))\n",
    "        all_curves_per_Sall_chunk.append(S_all_processed.shape[0])\n",
    "        if S_all_chunk==num_S_all_chunks:\n",
    "            unfiltered_curves_per_Sall_chunk.append(np.sum(dimers_by_param_set[(S_all_chunk*max_networks_per_S_all):,:]))\n",
    "        else:\n",
    "            unfiltered_curves_per_Sall_chunk.append(np.sum(dimers_by_param_set[(S_all_chunk*max_networks_per_S_all):((S_all_chunk+1)*max_networks_per_S_all),:]))\n",
    "        nonzero_curves_per_Sall_chunk.append(np.where(~np.all(S_all_processed==round_low_concs_to,axis=1))[0].shape[0])\n",
    "        filtered_curves_per_Sall_chunk.append(S_all_processed_filtered.shape[0])\n",
    "\n",
    "        # Calculate map between original parameter sets and those kept after filtering\n",
    "        # Allows us to trace curves in clusters back to original parameter sets\n",
    "        univ_after_filter = np.where(dynamic_ranges>=min_dynamic_range)[0]\n",
    "        num_univ_after_filter = np.where(dynamic_ranges>=min_dynamic_range)[0].shape[0]\n",
    "        univ_filter_map = np.concatenate((np.expand_dims(univ_after_filter, axis=1),np.expand_dims(np.arange(num_univ_after_filter),axis=1)),axis=1)\n",
    "        np.save(str(pathlib.Path(specific_out_dir,f'univ_filter_map_{S_all_chunk}.npy')), univ_filter_map)\n",
    "    S_all_filenames_by_m.append(S_all_processed_filtered_files)\n",
    "    all_curves_per_Sall_chunk_by_m.append(all_curves_per_Sall_chunk)\n",
    "    unfiltered_curves_per_Sall_chunk_by_m.append(unfiltered_curves_per_Sall_chunk)\n",
    "    nonzero_curves_per_Sall_chunk_by_m.append(nonzero_curves_per_Sall_chunk)\n",
    "    filtered_curves_per_Sall_chunk_by_m.append(filtered_curves_per_Sall_chunk)\n",
    "\n",
    "np.save(str(pathlib.Path(out_dir,'S_all_filenames_by_m.npy')),S_all_filenames_by_m)\n",
    "np.save(str(pathlib.Path(out_dir,'all_curves_per_Sall_chunk_by_m.npy')),all_curves_per_Sall_chunk_by_m)\n",
    "np.save(str(pathlib.Path(out_dir,'unfiltered_curves_per_Sall_chunk_by_m.npy')),unfiltered_curves_per_Sall_chunk_by_m)\n",
    "np.save(str(pathlib.Path(out_dir,'nonzero_curves_per_Sall_chunk_by_m.npy')),nonzero_curves_per_Sall_chunk_by_m)\n",
    "np.save(str(pathlib.Path(out_dir,'filtered_curves_per_Sall_chunk_by_m.npy')),filtered_curves_per_Sall_chunk_by_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
